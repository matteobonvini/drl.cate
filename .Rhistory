num_grid = 100,
nsplits = nsplits,
mu1.x = mu1.x,
mu0.x = mu0.x,
pi.x = pi.x,
drl = drl,
drl.ite = drl.ite)
})
# Fit the CATE
system.time({
cate.fit <- cate(data_frame = data.est, learner = "dr",
x_names = xnames,
y_name = "complication",
a_name = "treat",
v_names = eff.modif,
num_grid = 100,
nsplits = nsplits,
mu1.x = mu1.x,
mu0.x = mu0.x,
pi.x = pi.x,
drl.v = drl,
drl.v = drl.ite)
})
cate.fit <- cate(data_frame = data.est, learner = "dr",
x_names = xnames,
y_name = "complication",
a_name = "treat",
v_names = eff.modif,
num_grid = 100,
nsplits = nsplits,
mu1.x = mu1.x,
mu0.x = mu0.x,
pi.x = pi.x,
drl.x = drl,
drl.v = drl.ite)
# Fit the CATE
system.time({
cate.fit <- cate(data_frame = data.est, learner = "dr",
x_names = xnames,
y_name = "complication",
a_name = "treat",
v_names = eff.modif,
num_grid = 100,
nsplits = nsplits,
mu1.x = mu1.x,
mu0.x = mu0.x,
pi.x = pi.x,
drl.x = drl,
drl.v = drl.ite)
})
# calculate the residuals
residuals <- cate.fit$pseudo.y[[1]][, 1] - cate.fit$ites_v[[1]][, 1]
v.mat <- model.matrix(as.formula(cate.fit$drl.form), data = cate.fit$v)
v0.mat <- model.matrix(as.formula(cate.fit$drl.form), data = cate.fit$v0.long)
# design matrices for second stage regression; these will depend on drl.
cate.obj <- list(predictions = cate.fit$est[[1]][, 1],
fitted = cate.fit$ites_v[[1]][, 1],
residuals = residuals,
design.mat = v.mat,
design.mat.v0 = v0.mat,
v0 = cate.fit$v0)
lvl.set <- cate_lvl_set(theta = 0, cate.obj = cate.obj,
set_type = "upper", se = TRUE, B = 1e4, alpha = 0.05)
pseudo <- .get.pseudo.y(y = data$complication,
a = data$treat,
x = data[, xnames],
v1 = data[, eff.modif],
v2 = data[, eff.modif],
nsplits = 2,
mu0.x = mu0.x, mu1.x = mu1.x, pi.x = pi.x,
cond.dens = cond.dens, cate.w = cate.w)
setwd("/Users/matteobonvini/Desktop/cate_estimation/drl.cate")
library(roxygen2)
rm(list = c("drkern", "predict.drkern", "cate", "cate.lvl.set"))
roxygenize()
document()
library(devtools)
remove.packages(rlang)
install.packages("rlang")
library(devtools)
document()
document()
install_github("matteobonvini/drl.cate")
install_github("matteobonvini/drl.cate")
roxygenize()
library(roxygen2)
rm(list = c("drkern", "predict.drkern", "cate", "cate.lvl.set"))
roxygenize()
document()
library(devtools)
install_github("matteobonvini/drl.cate")
setwd("/Users/matteobonvini/Desktop/cate_estimation/drl.cate")
library(roxygen2)
rm(list = c("drkern", "predict.drkern", "cate", "cate.lvl.set"))
roxygenize()
document()
roxygenize()
install_github("matteobonvini/drl.cate")
library(drl.cate)
# library(devtools)
# install_github("matteobonvini/drl.cate", force=TRUE)
#install.packages("expm")
library(expm)
library(dplyr)
library(drl.cate)
library(tidyverse)
rm(list=ls())
source("/Users/matteobonvini/Dropbox/Hetero Effects/analysis/level-sets/nuisance-funcs-ml.R")
set.seed(2022)
n <- 2000
nsplits <- 5
bin_vars <- as.data.frame(matrix(rbinom(n * 44, 1, 0.5), ncol = 44))
colnames(bin_vars) <- c("male", "hisp", "white", "afam", "other",
"p_cat1", "p_cat2", "p_cat3", "p_cat4", "p_cat5",
"ynel1", "ynel2","ynel3","ynel4", "ynel5","ynel6", "ynel7",
"ynel8","ynel9", "ynel10","ynel11","ynel12","ynel13","ynel14",
"ynel15","ynel16","ynel17", "ynel18","ynel19","ynel20","ynel21",
"ynel22","ynel23","ynel24","ynel25","ynel26", "ynel27","ynel28",
"ynel29","ynel30","ynel31", "adm_type1", "adm_type2", "adm_type3")
data <- data.frame(age = sample(18:80, n, replace = TRUE),
comorb = sample(1:8, n, replace = TRUE),
agenow = sample(20:67, n, replace = TRUE),
yearsexp = sample(1:45, n, replace = TRUE))
expit <- function(x) exp(x) / (1 + exp(x))
# probs <- 0.4 + data$treat * I(data$age <= 50) * 0.2 +
#   - data$treat * I(data$comorb > 4) * 0.3
probs.a <- expit(2 - data$age/30)
range(probs.a )
data$treat <- rbinom(n, 1, probs.a)
probs <- expit(-2 + data$age/30)
range(probs)
complication1 <- rbinom(n, 1, probs)
complication0 <- rbinom(n, 1, 0.2)
data$complication <- data$treat*complication1 +
(1-data$treat)*complication0
data <- cbind(data, bin_vars)
# Prep The Data
eff.modif <- c("age","comorb")
xnames <- c("age","comorb",
"male", "hisp", "white", "afam", "other",
"p_cat1", "p_cat2", "p_cat3", "p_cat4", "p_cat5",
"ynel1", "ynel2","ynel3","ynel4", "ynel5","ynel6", "ynel7",
"ynel8","ynel9", "ynel10","ynel11","ynel12","ynel13","ynel14",
"ynel15","ynel16","ynel17", "ynel18","ynel19","ynel20","ynel21",
"ynel22","ynel23","ynel24","ynel25","ynel26", "ynel27","ynel28",
"ynel29","ynel30","ynel31", "adm_type1", "adm_type2", "adm_type3",
"agenow", "yearsexp")
data.est <- data
# Fit the CATE
system.time({
cate.fit <- cate(data_frame = data.est, learner = "dr",
x_names = xnames,
y_name = "complication",
a_name = "treat",
v_names = eff.modif,
num_grid = 100,
nsplits = nsplits,
mu1.x = mu1.x,
mu0.x = mu0.x,
pi.x = pi.x,
drl.x = drl,
drl.v = drl.ite)
})
# calculate the residuals
residuals <- cate.fit$pseudo.y[[1]][, 1] - cate.fit$ites_v[[1]][, 1]
v.mat <- model.matrix(as.formula(cate.fit$drl.form), data = cate.fit$v)
v0.mat <- model.matrix(as.formula(cate.fit$drl.form), data = cate.fit$v0.long)
# design matrices for second stage regression; these will depend on drl.
cate.obj <- list(predictions = cate.fit$est[[1]][, 1],
fitted = cate.fit$ites_v[[1]][, 1],
residuals = residuals,
design.mat = v.mat,
design.mat.v0 = v0.mat,
v0 = cate.fit$v0)
lvl.set <- cate_lvl_set(theta = 0, cate.obj = cate.obj,
set_type = "upper", se = TRUE, B = 1e4, alpha = 0.05)
pseudo <- .get.pseudo.y(y = data$complication,
a = data$treat,
x = data[, xnames],
v1 = data[, eff.modif],
v2 = data[, eff.modif],
nsplits = 2,
mu0.x = mu0.x, mu1.x = mu1.x, pi.x = pi.x,
cond.dens = cond.dens, cate.w = cate.w)
pseudo <- get_pseudo_outcome(y = data$complication,
a = data$treat,
x = data[, xnames],
v1 = data[, eff.modif],
v2 = data[, eff.modif],
nsplits = 2,
mu0.x = mu0.x, mu1.x = mu1.x, pi.x = pi.x,
cond.dens = cond.dens, cate.w = cate.w)
# specify the nuisance regression functions estimator
# SL.lib <- c("SL.mean", "SL.lm", "SL.glm", "SL.gam", "SL.polymars",
#             "SL.glm.interaction", "SL.glmnet")
#SL.lib <- c("SL.mean", "SL.lm")
mu1.x <- function(y, a, x, new.x) {
# fit <- SuperLearner(Y = y[a == 1], X = x[a == 1, , drop = FALSE],
#                     newX = new.x,
#                     family = binomial(), # set to binomial() if outcome is binary
#                     SL.library = SL.lib)
# return(predict(fit, new.x, onlySL = TRUE)$pred)
fit <- ranger::ranger(y = as.factor(y[a == 1]), x = x[a == 1, , drop = FALSE],
probability = TRUE)
return(predict(fit, data = new.x)$predictions[, "1"])
}
mu0.x <- function(y, a, x, new.x) {
# fit <- SuperLearner(Y = y[a == 0], X = x[a == 0, , drop = FALSE],
#                     newX = new.x,
#                     family = binomial(), # set to binomial() if outcome is binary
#                     SL.library = SL.lib)
# return(predict(fit, newdata = new.x, onlySL = TRUE)$pred)
fit <- ranger::ranger(y = as.factor(y[a == 0]), x = x[a == 0, , drop = FALSE],
probability = TRUE)
return(predict(fit, data = new.x)$predictions[, "1"])
}
pi.x <- function(a, x, new.x) {
# fit <- SuperLearner(Y = a, X = x, newX = new.x,
#                     family = binomial(),
#                     SL.library = SL.lib)
# return(predict(fit, newdata = new.x, onlySL = TRUE)$pred)
fit <- ranger::ranger(y = as.factor(a), x = x, probability = TRUE)
# fit <- gam::gam(y ~ s(age), data = cbind(data.frame(y = a), x), family=binomial)
# preds <- predict(fit, newdata = as.data.frame(new.x))
return(predict(fit, data = new.x)$predictions[, "1"])
# return(preds)
}
# specify second-stage regression
# the NAs are just place holders in case in the future we return upper and lower
# confidence bands.
drl <- function(y, x, new.x) {
require(splines)
# drl.form <- paste0("y ~ poly(", colnames(x)[1], ", 3, raw = TRUE) + poly(",
#                    colnames(x)[2], ", 3, raw = TRUE)")
# drl.form <- paste0("~ as.factor(", colnames(x)[2], ")*",
#                    "poly(", colnames(x)[1], ", 2, raw = TRUE)")
# drl.form <- paste0("~ as.factor(", colnames(x)[2], ")*",
#                    "I(", colnames(x)[1], ">= 50)")
# drl.form <- paste0("~ s(", colnames(x)[1], ", by = as.factor(", colnames(x)[2], "))")
drl.form <- paste0("~ as.factor(", colnames(x)[2], ")*",
"bs(", colnames(x)[1], ", df = 6, degree = 3)")
fit <- lm(as.formula(paste0("y", drl.form)), data = cbind(data.frame(y = y), x))
res <- cbind(predict(fit, newdata = as.data.frame(new.x)), NA, NA)
return(list(res = res, model = fit, drl.form = drl.form))
}
# specify second-stage regression to compute ITEs. E(Y_1 - Y_0 | V_i)
drl.ite <- function(y, x, new.x) {
require(splines)
# drl.form <- paste0("y ~ poly(", colnames(x)[1], ", 3, raw = TRUE) + poly(",
# colnames(x)[2], ", 3, raw = TRUE)")
# drl.form <- paste0("~ as.factor(", colnames(x)[2], ")*",
# "poly(", colnames(x)[1], ", 2, raw = TRUE)")
# drl.form <- paste0("~ as.factor(", colnames(x)[2], ")*",
#                    "I(", colnames(x)[1], ">= 50)")
# fit <- lm(as.formula(paste0("y", drl.form)), data = cbind(data.frame(y = y), x))
drl.form <- paste0("~ as.factor(", colnames(x)[2], ")*",
"bs(", colnames(x)[1], ", df = 6, degree = 3)")
fit <- lm(as.formula(paste0("y", drl.form)), data = cbind(data.frame(y = y), x))
res <- cbind(predict(fit, newdata = as.data.frame(new.x)), NA, NA)
return(list(res = res, model = fit, drl.form = drl.form))
}
cond.dens <- function(v1, v2, new.v1, new.v2) {
require(splines)
drl.form <- paste0("~ bs(", colnames(v2)[1], ", df = 6, degree = 3)")
mean.fit <- lm(as.formula(paste0("y", drl.form)),
data = cbind(data.frame(y = v1), v2))
pred.means <- predict(mean.fit, newdata = as.data.frame(new.v2))
var.fit <- lm(as.formula(paste0("y", drl.form)),
data = cbind(data.frame(y = (v1 - pred.means)^2), v2))
pred.vars <- predict(var.fit, newdata = as.data.frame(v2))
stopifnot(all(pred.vars >= 0))
v1.std.tr <- (v1 - pred.means) / sqrt(pred.vars)
v1.std.te <- (new.v1 - predict(mean.fit, newdata = as.data.frame(new.v2))) /
sqrt(predict(var.fit, newdata = as.data.frame(new.v2)))
out <- approx(density(v1.std.tr)$x, density(v1.std.tr)$y, xout = v1.std.te)$y /
sqrt(predict(var.fit, newdata = as.data.frame(new.v2)))
return(out)
}
cate.w <- function(tau, w, new.w) {
cate.w.form <- paste0("~ as.factor(", colnames(w)[2], ")*",
"bs(", colnames(w)[1], ", df = 6, degree = 3)")
fit <- lm(as.formula(paste0("y", cate.w.form)),
data = cbind(data.frame(y = tau), w))
res <- predict(fit, newdata = as.data.frame(new.w))
return(list(res = res, model = fit, cate.w.form = cate.w.form))
}
pseudo <- get_pseudo_outcome(y = data$complication,
a = data$treat,
x = data[, xnames],
v1 = data[, eff.modif],
v2 = data[, eff.modif],
nsplits = 2,
mu0.x = mu0.x, mu1.x = mu1.x, pi.x = pi.x,
cond.dens = cond.dens, cate.w = cate.w)
res <- debiased_inference(A = data[, eff.modif[1]],
pseudo.out = pseudo$pd.out[, eff.modif[1]],
tau = 1,
muhat.mat = pseudo$theta.mat,
mhat.obs = pseudo$theta.bar,
debias = FALSE,
bandwidth.method="LOOCV",
kernel.type = "epa",
bw.seq = 10)
setwd("/Users/matteobonvini/Desktop/cate_estimation/drl.cate")
library(roxygen2)
rm(list = c("drkern", "predict.drkern", "cate", "cate.lvl.set"))
roxygenize()
document()
library(devtools)
document()
install_github("matteobonvini/drl.cate")
library(drl.cate)
res <- debiased_inference(A = data[, eff.modif[1]],
pseudo.out = pseudo$pd.out[, eff.modif[1]],
tau = 1,
muhat.mat = pseudo$theta.mat,
mhat.obs = pseudo$theta.bar,
debias = FALSE,
bandwidth.method="LOOCV",
kernel.type = "epa",
bw.seq = 10)
plot(x = data[, eff.modif[1]], y = pseudo$pd.out[, eff.modif[1]])
plot_debiased_curve(res)
plot(x = res$eval.pts, y = res$theta, ylim = c(-5, 5))
segments(x0 = res$eval.pts, y0 = res$ci.ll.unif, y1 = res$ci.ul.unif, col = "red")
abline(h = 0, col = "blue")
points(x = data[, eff.modif[1]], y = pseudo$pd.out[, eff.modif[1]],
col = "green")
res$b
res$h
res <- debiased_inference(A = data[, eff.modif[2]],
pseudo.out = pseudo$cate.out,
tau = 1,
muhat.mat = matrix(0, nrow(data)^2),
mhat.obs = rep(0, nrow(data)),
debias = FALSE,
bandwidth.method="LOOCV",
kernel.type = "epa",
bw.seq = 4)
plot_debiased_curve(res)
plot(x = res$eval.pts, y = res$theta, ylim = c(-3, 3))
segments(x0 = res$eval.pts, y0 = res$ci.ll.unif, y1 = res$ci.ul.unif, col = "red")
abline(h = 0, col = "blue")
points(x = data[, eff.modif[2]], y = pseudo$cate.out, col = "blue")
res$b
res$h
res <- debiased_inference(A = data[, eff.modif[1]],
pseudo.out = pseudo$pd.out[, eff.modif[1]],
tau = 1,
muhat.mat = pseudo$theta.mat,
mhat.obs = pseudo$theta.bar,
debias = TRUE,
bandwidth.method="LOOCV",
kernel.type = "epa",
bw.seq = 10)
plot(x = data[, eff.modif[1]], y = pseudo$pd.out[, eff.modif[1]])
plot_debiased_curve(res)
plot(x = res$eval.pts, y = res$theta, ylim = c(-5, 5))
segments(x0 = res$eval.pts, y0 = res$ci.ll.unif, y1 = res$ci.ul.unif, col = "red")
abline(h = 0, col = "blue")
points(x = data[, eff.modif[1]], y = pseudo$pd.out[, eff.modif[1]],
col = "green")
res$b
res$h
rep(NA, 6000^2)
rep(NA, 60000^2)
n <- 60000
nsplits <- 5
bin_vars <- as.data.frame(matrix(rbinom(n * 44, 1, 0.5), ncol = 44))
colnames(bin_vars) <- c("male", "hisp", "white", "afam", "other",
"p_cat1", "p_cat2", "p_cat3", "p_cat4", "p_cat5",
"ynel1", "ynel2","ynel3","ynel4", "ynel5","ynel6", "ynel7",
"ynel8","ynel9", "ynel10","ynel11","ynel12","ynel13","ynel14",
"ynel15","ynel16","ynel17", "ynel18","ynel19","ynel20","ynel21",
"ynel22","ynel23","ynel24","ynel25","ynel26", "ynel27","ynel28",
"ynel29","ynel30","ynel31", "adm_type1", "adm_type2", "adm_type3")
data <- data.frame(age = sample(18:80, n, replace = TRUE),
comorb = sample(1:8, n, replace = TRUE),
agenow = sample(20:67, n, replace = TRUE),
yearsexp = sample(1:45, n, replace = TRUE))
expit <- function(x) exp(x) / (1 + exp(x))
# probs <- 0.4 + data$treat * I(data$age <= 50) * 0.2 +
#   - data$treat * I(data$comorb > 4) * 0.3
probs.a <- expit(2 - data$age/30)
range(probs.a )
data$treat <- rbinom(n, 1, probs.a)
probs <- expit(-2 + data$age/30)
range(probs)
complication1 <- rbinom(n, 1, probs)
complication0 <- rbinom(n, 1, 0.2)
data$complication <- data$treat*complication1 +
(1-data$treat)*complication0
data <- cbind(data, bin_vars)
# Prep The Data
eff.modif <- c("age","comorb")
xnames <- c("age","comorb",
"male", "hisp", "white", "afam", "other",
"p_cat1", "p_cat2", "p_cat3", "p_cat4", "p_cat5",
"ynel1", "ynel2","ynel3","ynel4", "ynel5","ynel6", "ynel7",
"ynel8","ynel9", "ynel10","ynel11","ynel12","ynel13","ynel14",
"ynel15","ynel16","ynel17", "ynel18","ynel19","ynel20","ynel21",
"ynel22","ynel23","ynel24","ynel25","ynel26", "ynel27","ynel28",
"ynel29","ynel30","ynel31", "adm_type1", "adm_type2", "adm_type3",
"agenow", "yearsexp")
data.est <- data
# Fit the CATE
system.time({
cate.fit <- cate(data_frame = data.est, learner = "dr",
x_names = xnames,
y_name = "complication",
a_name = "treat",
v_names = eff.modif,
num_grid = 100,
nsplits = nsplits,
mu1.x = mu1.x,
mu0.x = mu0.x,
pi.x = pi.x,
drl.x = drl,
drl.v = drl.ite)
})
# calculate the residuals
residuals <- cate.fit$pseudo.y[[1]][, 1] - cate.fit$ites_v[[1]][, 1]
v.mat <- model.matrix(as.formula(cate.fit$drl.form), data = cate.fit$v)
v0.mat <- model.matrix(as.formula(cate.fit$drl.form), data = cate.fit$v0.long)
# design matrices for second stage regression; these will depend on drl.
cate.obj <- list(predictions = cate.fit$est[[1]][, 1],
fitted = cate.fit$ites_v[[1]][, 1],
residuals = residuals,
design.mat = v.mat,
design.mat.v0 = v0.mat,
v0 = cate.fit$v0)
lvl.set <- cate_lvl_set(theta = 0, cate.obj = cate.obj,
set_type = "upper", se = TRUE, B = 1e4, alpha = 0.05)
pseudo <- get_pseudo_outcome(y = data$complication,
a = data$treat,
x = data[, xnames],
v1 = data[, eff.modif],
v2 = data[, eff.modif],
nsplits = 2,
mu0.x = mu0.x, mu1.x = mu1.x, pi.x = pi.x,
cond.dens = cond.dens, cate.w = cate.w)
y = data$complication
a = data$treat
x = data[, xnames]
v1 = data[, eff.modif]
v2 = data[, eff.modif]
n <- length(y)
s <- sample(rep(1:nsplits, ceiling(n / nsplits))[1:n])
mu0hat <- mu1hat <- pihat <- tauhat <- cate.out <- rep(NA, n)
cate.out.folds <- pd.out.folds <- vector("list", nsplits)
theta.bar <- ghat <- tauhat.w <- rep(NA, n)
theta.mat <- matrix(NA, ncol = n, nrow = n)
pd.out <- matrix(NA, ncol = ncol(v1), nrow = n,
dimnames = list(NULL, colnames(v1)))
k <- 1
test.idx <- k == s
train.idx <- k != s
n.te <- sum(test.idx)
n.tr <- sum(train.idx)
nsplits <- 2
n <- length(y)
s <- sample(rep(1:nsplits, ceiling(n / nsplits))[1:n])
mu0hat <- mu1hat <- pihat <- tauhat <- cate.out <- rep(NA, n)
cate.out.folds <- pd.out.folds <- vector("list", nsplits)
theta.bar <- ghat <- tauhat.w <- rep(NA, n)
theta.mat <- matrix(NA, ncol = n, nrow = n)
pd.out <- matrix(NA, ncol = ncol(v1), nrow = n,
dimnames = list(NULL, colnames(v1)))
if(!is.null(v2)) {
theta.bar <- ghat <- tauhat.w <- rep(NA, n)
theta.mat <- matrix(NA, ncol = n, nrow = n)
pd.out <- matrix(NA, ncol = ncol(v1), nrow = n,
dimnames = list(NULL, colnames(v1)))
}
test.idx <- k == s
train.idx <- k != s
n.te <- sum(test.idx)
n.tr <- sum(train.idx)
a.tr <- a[train.idx]
y.tr <- y[train.idx]
x.tr <- x[train.idx, , drop = FALSE]
a.te <- a[test.idx]
y.te <- y[test.idx]
x.te <- x[test.idx, , drop = FALSE]
mu0hat.vals <- mu0.x(y = y.tr, a = a.tr, x = x.tr,
new.x = rbind(x.te, x.tr))
mu0hat[test.idx] <- mu0hat.vals[1:n.te]
mu1hat.vals <- mu1.x(y = y.tr, a = a.tr, x = x.tr,
new.x = rbind(x.te, x.tr))
mu1hat[test.idx] <- mu1hat.vals[1:n.te]
pihat[test.idx] <- pi.x(a = a.tr, x = x.tr, new.x = x.te)
tauhat[test.idx] <- mu1hat[test.idx] - mu0hat[test.idx]
cate.out[test.idx] <- a.te / pihat[test.idx] * (y.te - mu1hat[test.idx]) -
(1-a.te) / (1 - pihat[test.idx]) * (y.te - mu0hat[test.idx]) +
tauhat[test.idx]
j <- 1
v1.j <- v1[, j]
v2.not.v1.j <- v2[, colnames(v2) != colnames(v1)[j], drop = FALSE]
v1.tr <- v1.j[train.idx]
v2.tr <- v2[train.idx, , drop = FALSE]
v2.not.v1.j.tr <- v2.not.v1.j[train.idx, , drop = FALSE]
v1.te <- v1.j[test.idx]
v2.te <- v2[test.idx, , drop = FALSE]
v2.not.v1.j.te <- v2.not.v1.j[test.idx, , drop = FALSE]
w.long.test <- cbind(v1[test.idx, j, drop = FALSE],
v2.not.v1.j.te[rep(1:n.te, n.te), , drop = FALSE])
length(rep(1:n.te, n.te))
tmp <- rep(NA, 30000^2)
tmp <- rep(NA, 3000^2)
usethis::edit_r_environ()
v2.not.v1.j.te[rep(1:n.te, n.te), , drop = FALSE]
memory.limit()
