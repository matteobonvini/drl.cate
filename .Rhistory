basis.x <- matrix(NA, nrow = n.te, ncol = (order_basis + 1)^d)
basis.additive <- array(dim = c(n.te, d, order_basis + 1))
basis.0 <- rep(NA, (order_basis + 1)^d)
basis.additive.0 <- array(dim = c(1, d, order_basis + 1))
for(b in 1:(order_basis + 1)) {
basis.additive[, , b] <- apply(as.matrix(x.te) - x0.mat, 2, basis, j = b)
basis.additive.0[1, , b] <- basis(0, j = b)
}
for(tb in 1:(order_basis + 1)^d) {
basis.x[, tb] <- apply(sapply(1:d,
function(l) basis.additive[, l, tensor.idx[tb, l]],
simplify = TRUE), 1, prod)
tmp <- sapply(1:d,
function(l) basis.additive.0[, l, tensor.idx[tb, l]],
simplify = TRUE)
basis.0[tb] <- prod(as.matrix(tmp, nrow = 1, ncol = d))
}
pseudo.x <- matrix(rep((a.te - pi1hat), (order_basis + 1)^d), ncol = (order_basis + 1)^d) * basis.x
est[j, k] <- sum(basis.0 * coef(lm(pseudo.y ~ -1 + pseudo.x, weights = ww)))
}
}
out <- list(est = apply(est, 1, mean), fold_k_est = est)
return(out)
}
# Examples for the DR-Learner and Lp-R-Learner.
set.seed(1234)
require(orthopolynom)
require(SuperLearner)
# some helper functions
expit <- function(x) exp(x) / (1 + exp(x))
logit <- function(x) log( x / (1 - x))
# functions to estimate nuisance regressions
mu1.x <- function(y, x, new.x, sl.lib = c("SL.mean", "SL.lm", "SL.gam",
"SL.polymars", "SL.rpart")){
fit <- SuperLearner::SuperLearner(Y = y, X = x, SL.library = sl.lib,
newX = new.x)
return(fit$SL.predict)
}
mu.x <- mu0.x <- mu1.x
pi.x <- function(a, x, new.x, sl.lib = c("SL.mean", "SL.glm", "SL.ranger",
"SL.rpart")){
fit <- SuperLearner::SuperLearner(Y = a, X = x, SL.library = sl.lib,
newX = new.x, family = binomial())
return(fit$SL.predict)
}
drl.x <- function(y, x, new.x, sl.lib = c("SL.mean", "SL.lm", "SL.gam",
"SL.loess", "SL.glm", "SL.polymars",
"SL.rpart")){
fit <- SuperLearner::SuperLearner(Y = y, X = x, SL.library = sl.lib,
newX = new.x)
return(fit$SL.predict)
}
order_basis <- 2
lp <- legendre.polynomials(n = order_basis, normalized = TRUE)
lp.fns <- lapply(1:(order_basis + 1), function(u) as.function(lp[[u]]))
basis.legendre <- function(x, j) {
return(lp.fns[[j]](x))
}
h <- 0.1
kernel.gaussian <- function(x, x0) {
tmp <- function(u) prod(dnorm( as.matrix((u - x0)) / h) / h)
out <- apply(x, 1, tmp)
return(out)
}
# some generic parameters
n <- 2000
#################################
# Example 1 from Kennedy (2020) #
#################################
x0 <- cbind(seq(-0.8, 0.8, length.out = 101),
seq(-0.8, 0.8, length.out = 101))
mu0_true <- function(y, x, new.x) {
u <- new.x[, 1]
(u <= -0.5) * 0.5 * (u + 2)^2 +
(u * 0.5 + 0.875) * (u > -0.5 & u < 0) +
(u > 0 & u < 0.5) * (-5 * (u - 0.2)^2 + 1.075) +
(u > .5) * (u + 0.125)
}
mu1_true <- function(y, x, new.x) {
mu0_true(y, x, new.x)
}
pix_true <- function(a, x, new.x){
u <- new.x[, 1]
0.1 + 0.8 * (u > 0)
}
gen_data <- function(n){
x <- data.frame(x = runif(n, -1, 1))
a <- rbinom(n, 1, pix_true(a = NULL, x = NULL, new.x = x))
y <- a * mu1_true(y = NULL, x = NULL, new.x = x) +
(1 - a) * mu0_true(y = NULL, x = NULL, new.x = x) +
rnorm(n, sd = (0.2 - 0.1 * cos(2 * pi * x$x)))
dat <- data.frame(y = y, a = a, x1 = x$x, x2 = runif(n, -1, 1))
return(dat)
}
dat <- gen_data(n)
res_drl <- dr_learner(x0 = x0, y = dat$y, a = dat$a, x = dat[, -c(1:2), drop = FALSE],
drl.x = drl.x, mu1.x = mu1.x, mu0.x = mu0.x, pi.x = pi.x,
nsplits = 5)
res_lprl <- lp_r_learner(x0 = x0, y = dat$y, a = dat$a, x = dat[, -c(1:2), drop = FALSE],
mu.x = mu.x, pi.x = pi.x, basis = basis.legendre,
order_basis = order_basis, kernel = kernel.gaussian)
oracle <- dr_learner(x0 = x0, y = dat$y, a = dat$a, x = dat[, -c(1:2), drop = FALSE],
drl.x = drl.x, mu1.x = mu1_true, mu0.x = mu0_true,
pi.x = pix_true, nsplits = 5)
x0 <- x0[, 1]
plot(x0, y = rep(0, length(x0)), type = "l", col = "red", ylim = c(-1.5, 1.5),
ylab = "E(Y^1 - Y^0)", xlab = "X0")
lines(x0, y = oracle$est, col = "blue")
lines(x0, y = res_drl$est, col = "black")
lines(x0, y = res_lprl$est, col = "pink")
legend(0.15, 1.5, legend = c("Truth", "Oracle", "DR-Learner", "Lp-R-Learner"),
col = c("red", "blue", "black", "pink"), cex = 0.8, lty = 1)
x0 <- x0[, 2]
plot(x0, y = rep(0, length(x0)), type = "l", col = "red", ylim = c(-1.5, 1.5),
ylab = "E(Y^1 - Y^0)", xlab = "X0")
lines(x0, y = oracle$est, col = "blue")
lines(x0, y = res_drl$est, col = "black")
lines(x0, y = res_lprl$est, col = "pink")
legend(0.15, 1.5, legend = c("Truth", "Oracle", "DR-Learner", "Lp-R-Learner"),
col = c("red", "blue", "black", "pink"), cex = 0.8, lty = 1)
# Examples for the DR-Learner and Lp-R-Learner.
set.seed(1234)
require(orthopolynom)
require(SuperLearner)
# some helper functions
expit <- function(x) exp(x) / (1 + exp(x))
logit <- function(x) log( x / (1 - x))
# functions to estimate nuisance regressions
mu1.x <- function(y, x, new.x, sl.lib = c("SL.mean", "SL.lm", "SL.gam",
"SL.polymars", "SL.rpart")){
fit <- SuperLearner::SuperLearner(Y = y, X = x, SL.library = sl.lib,
newX = new.x)
return(fit$SL.predict)
}
mu.x <- mu0.x <- mu1.x
pi.x <- function(a, x, new.x, sl.lib = c("SL.mean", "SL.glm", "SL.ranger",
"SL.rpart")){
fit <- SuperLearner::SuperLearner(Y = a, X = x, SL.library = sl.lib,
newX = new.x, family = binomial())
return(fit$SL.predict)
}
drl.x <- function(y, x, new.x, sl.lib = c("SL.mean", "SL.lm", "SL.gam",
"SL.loess", "SL.glm", "SL.polymars",
"SL.rpart")){
fit <- SuperLearner::SuperLearner(Y = y, X = x, SL.library = sl.lib,
newX = new.x)
return(fit$SL.predict)
}
order_basis <- 5
lp <- legendre.polynomials(n = order_basis, normalized = TRUE)
lp.fns <- lapply(1:(order_basis + 1), function(u) as.function(lp[[u]]))
basis.legendre <- function(x, j) {
return(lp.fns[[j]](x))
}
h <- 0.1
kernel.gaussian <- function(x, x0) {
tmp <- function(u) prod(dnorm( as.matrix((u - x0)) / h) / h)
out <- apply(x, 1, tmp)
return(out)
}
# some generic parameters
n <- 2000
#################################
# Example 1 from Kennedy (2020) #
#################################
x0 <- cbind(seq(-0.8, 0.8, length.out = 101),
seq(-0.8, 0.8, length.out = 101))
mu0_true <- function(y, x, new.x) {
u <- new.x[, 1]
(u <= -0.5) * 0.5 * (u + 2)^2 +
(u * 0.5 + 0.875) * (u > -0.5 & u < 0) +
(u > 0 & u < 0.5) * (-5 * (u - 0.2)^2 + 1.075) +
(u > .5) * (u + 0.125)
}
mu1_true <- function(y, x, new.x) {
mu0_true(y, x, new.x)
}
pix_true <- function(a, x, new.x){
u <- new.x[, 1]
0.1 + 0.8 * (u > 0)
}
gen_data <- function(n){
x <- data.frame(x = runif(n, -1, 1))
a <- rbinom(n, 1, pix_true(a = NULL, x = NULL, new.x = x))
y <- a * mu1_true(y = NULL, x = NULL, new.x = x) +
(1 - a) * mu0_true(y = NULL, x = NULL, new.x = x) +
rnorm(n, sd = (0.2 - 0.1 * cos(2 * pi * x$x)))
dat <- data.frame(y = y, a = a, x1 = x$x, x2 = runif(n, -1, 1))
return(dat)
}
dat <- gen_data(n)
res_drl <- dr_learner(x0 = x0, y = dat$y, a = dat$a, x = dat[, -c(1:2), drop = FALSE],
drl.x = drl.x, mu1.x = mu1.x, mu0.x = mu0.x, pi.x = pi.x,
nsplits = 5)
res_lprl <- lp_r_learner(x0 = x0, y = dat$y, a = dat$a, x = dat[, -c(1:2), drop = FALSE],
mu.x = mu.x, pi.x = pi.x, basis = basis.legendre,
order_basis = order_basis, kernel = kernel.gaussian)
oracle <- dr_learner(x0 = x0, y = dat$y, a = dat$a, x = dat[, -c(1:2), drop = FALSE],
drl.x = drl.x, mu1.x = mu1_true, mu0.x = mu0_true,
pi.x = pix_true, nsplits = 5)
x0 <- x0[, 2]
plot(x0, y = rep(0, length(x0)), type = "l", col = "red", ylim = c(-1.5, 1.5),
ylab = "E(Y^1 - Y^0)", xlab = "X0")
lines(x0, y = oracle$est, col = "blue")
lines(x0, y = res_drl$est, col = "black")
lines(x0, y = res_lprl$est, col = "pink")
legend(0.15, 1.5, legend = c("Truth", "Oracle", "DR-Learner", "Lp-R-Learner"),
col = c("red", "blue", "black", "pink"), cex = 0.8, lty = 1)
x0 <- x0[, 1]
plot(x0, y = rep(0, length(x0)), type = "l", col = "red", ylim = c(-1.5, 1.5),
ylab = "E(Y^1 - Y^0)", xlab = "X0")
lines(x0, y = oracle$est, col = "blue")
lines(x0, y = res_drl$est, col = "black")
lines(x0, y = res_lprl$est, col = "pink")
# Examples for the DR-Learner and Lp-R-Learner.
set.seed(1234)
require(orthopolynom)
require(SuperLearner)
# some helper functions
expit <- function(x) exp(x) / (1 + exp(x))
logit <- function(x) log( x / (1 - x))
# functions to estimate nuisance regressions
mu1.x <- function(y, x, new.x, sl.lib = c("SL.mean", "SL.lm", "SL.gam",
"SL.polymars", "SL.rpart")){
fit <- SuperLearner::SuperLearner(Y = y, X = x, SL.library = sl.lib,
newX = new.x)
return(fit$SL.predict)
}
mu.x <- mu0.x <- mu1.x
pi.x <- function(a, x, new.x, sl.lib = c("SL.mean", "SL.glm", "SL.ranger",
"SL.rpart")){
fit <- SuperLearner::SuperLearner(Y = a, X = x, SL.library = sl.lib,
newX = new.x, family = binomial())
return(fit$SL.predict)
}
drl.x <- function(y, x, new.x, sl.lib = c("SL.mean", "SL.lm", "SL.gam",
"SL.loess", "SL.glm", "SL.polymars",
"SL.rpart")){
fit <- SuperLearner::SuperLearner(Y = y, X = x, SL.library = sl.lib,
newX = new.x)
return(fit$SL.predict)
}
order_basis <- 10
lp <- legendre.polynomials(n = order_basis, normalized = TRUE)
lp.fns <- lapply(1:(order_basis + 1), function(u) as.function(lp[[u]]))
basis.legendre <- function(x, j) {
return(lp.fns[[j]](x))
}
h <- 0.1
kernel.gaussian <- function(x, x0) {
tmp <- function(u) prod(dnorm( as.matrix((u - x0)) / h) / h)
out <- apply(x, 1, tmp)
return(out)
}
# some generic parameters
n <- 2000
#################################
# Example 1 from Kennedy (2020) #
#################################
x0 <- seq(-0.8, 0.8, length.out = 101)
mu0_true <- function(y, x, new.x) {
u <- new.x[, 1]
(u <= -0.5) * 0.5 * (u + 2)^2 +
(u * 0.5 + 0.875) * (u > -0.5 & u < 0) +
(u > 0 & u < 0.5) * (-5 * (u - 0.2)^2 + 1.075) +
(u > .5) * (u + 0.125)
}
mu1_true <- function(y, x, new.x) {
mu0_true(y, x, new.x)
}
pix_true <- function(a, x, new.x){
u <- new.x[, 1]
0.1 + 0.8 * (u > 0)
}
gen_data <- function(n){
x <- data.frame(x = runif(n, -1, 1))
a <- rbinom(n, 1, pix_true(a = NULL, x = NULL, new.x = x))
y <- a * mu1_true(y = NULL, x = NULL, new.x = x) +
(1 - a) * mu0_true(y = NULL, x = NULL, new.x = x) +
rnorm(n, sd = (0.2 - 0.1 * cos(2 * pi * x$x)))
dat <- data.frame(y = y, a = a, x1 = x$x)
return(dat)
}
dat <- gen_data(n)
res_drl <- dr_learner(x0 = x0, y = dat$y, a = dat$a, x = dat[, -c(1:2), drop = FALSE],
drl.x = drl.x, mu1.x = mu1.x, mu0.x = mu0.x, pi.x = pi.x,
nsplits = 5)
res_lprl <- lp_r_learner(x0 = x0, y = dat$y, a = dat$a, x = dat[, -c(1:2), drop = FALSE],
mu.x = mu.x, pi.x = pi.x, basis = basis.legendre,
order_basis = order_basis, kernel = kernel.gaussian)
oracle <- dr_learner(x0 = x0, y = dat$y, a = dat$a, x = dat[, -c(1:2), drop = FALSE],
drl.x = drl.x, mu1.x = mu1_true, mu0.x = mu0_true,
pi.x = pix_true, nsplits = 5)
x0 <- x0[, 1]
plot(x0, y = rep(0, length(x0)), type = "l", col = "red", ylim = c(-1.5, 1.5),
ylab = "E(Y^1 - Y^0)", xlab = "X0")
lines(x0, y = oracle$est, col = "blue")
lines(x0, y = res_drl$est, col = "black")
lines(x0, y = res_lprl$est, col = "pink")
legend(0.15, 1.5, legend = c("Truth", "Oracle", "DR-Learner", "Lp-R-Learner"),
col = c("red", "blue", "black", "pink"), cex = 0.8, lty = 1)
x0 <- x0
plot(x0, y = rep(0, length(x0)), type = "l", col = "red", ylim = c(-1.5, 1.5),
ylab = "E(Y^1 - Y^0)", xlab = "X0")
lines(x0, y = oracle$est, col = "blue")
lines(x0, y = res_drl$est, col = "black")
lines(x0, y = res_lprl$est, col = "pink")
legend(0.15, 1.5, legend = c("Truth", "Oracle", "DR-Learner", "Lp-R-Learner"),
col = c("red", "blue", "black", "pink"), cex = 0.8, lty = 1)
# Examples for the DR-Learner and Lp-R-Learner.
set.seed(1234)
require(orthopolynom)
require(SuperLearner)
# some helper functions
expit <- function(x) exp(x) / (1 + exp(x))
logit <- function(x) log( x / (1 - x))
# functions to estimate nuisance regressions
mu1.x <- function(y, x, new.x, sl.lib = c("SL.mean", "SL.lm", "SL.gam",
"SL.polymars", "SL.rpart")){
fit <- SuperLearner::SuperLearner(Y = y, X = x, SL.library = sl.lib,
newX = new.x)
return(fit$SL.predict)
}
mu.x <- mu0.x <- mu1.x
pi.x <- function(a, x, new.x, sl.lib = c("SL.mean", "SL.glm", "SL.ranger",
"SL.rpart")){
fit <- SuperLearner::SuperLearner(Y = a, X = x, SL.library = sl.lib,
newX = new.x, family = binomial())
return(fit$SL.predict)
}
drl.x <- function(y, x, new.x, sl.lib = c("SL.mean", "SL.lm", "SL.gam",
"SL.loess", "SL.glm", "SL.polymars",
"SL.rpart")){
fit <- SuperLearner::SuperLearner(Y = y, X = x, SL.library = sl.lib,
newX = new.x)
return(fit$SL.predict)
}
order_basis <- 30
lp <- legendre.polynomials(n = order_basis, normalized = TRUE)
lp.fns <- lapply(1:(order_basis + 1), function(u) as.function(lp[[u]]))
basis.legendre <- function(x, j) {
return(lp.fns[[j]](x))
}
h <- 0.1
kernel.gaussian <- function(x, x0) {
tmp <- function(u) prod(dnorm( as.matrix((u - x0)) / h) / h)
out <- apply(x, 1, tmp)
return(out)
}
# some generic parameters
n <- 2000
#################################
# Example 1 from Kennedy (2020) #
#################################
x0 <- seq(-0.8, 0.8, length.out = 101)
mu0_true <- function(y, x, new.x) {
u <- new.x[, 1]
(u <= -0.5) * 0.5 * (u + 2)^2 +
(u * 0.5 + 0.875) * (u > -0.5 & u < 0) +
(u > 0 & u < 0.5) * (-5 * (u - 0.2)^2 + 1.075) +
(u > .5) * (u + 0.125)
}
mu1_true <- function(y, x, new.x) {
mu0_true(y, x, new.x)
}
pix_true <- function(a, x, new.x){
u <- new.x[, 1]
0.1 + 0.8 * (u > 0)
}
gen_data <- function(n){
x <- data.frame(x = runif(n, -1, 1))
a <- rbinom(n, 1, pix_true(a = NULL, x = NULL, new.x = x))
y <- a * mu1_true(y = NULL, x = NULL, new.x = x) +
(1 - a) * mu0_true(y = NULL, x = NULL, new.x = x) +
rnorm(n, sd = (0.2 - 0.1 * cos(2 * pi * x$x)))
dat <- data.frame(y = y, a = a, x1 = x$x)
return(dat)
}
dat <- gen_data(n)
res_drl <- dr_learner(x0 = x0, y = dat$y, a = dat$a, x = dat[, -c(1:2), drop = FALSE],
drl.x = drl.x, mu1.x = mu1.x, mu0.x = mu0.x, pi.x = pi.x,
nsplits = 5)
res_lprl <- lp_r_learner(x0 = x0, y = dat$y, a = dat$a, x = dat[, -c(1:2), drop = FALSE],
mu.x = mu.x, pi.x = pi.x, basis = basis.legendre,
order_basis = order_basis, kernel = kernel.gaussian)
oracle <- dr_learner(x0 = x0, y = dat$y, a = dat$a, x = dat[, -c(1:2), drop = FALSE],
drl.x = drl.x, mu1.x = mu1_true, mu0.x = mu0_true,
pi.x = pix_true, nsplits = 5)
x0 <- x0
plot(x0, y = rep(0, length(x0)), type = "l", col = "red", ylim = c(-1.5, 1.5),
ylab = "E(Y^1 - Y^0)", xlab = "X0")
lines(x0, y = oracle$est, col = "blue")
lines(x0, y = res_drl$est, col = "black")
lines(x0, y = res_lprl$est, col = "pink")
legend(0.15, 1.5, legend = c("Truth", "Oracle", "DR-Learner", "Lp-R-Learner"),
col = c("red", "blue", "black", "pink"), cex = 0.8, lty = 1)
# Examples for the DR-Learner and Lp-R-Learner.
set.seed(1234)
require(orthopolynom)
require(SuperLearner)
# some helper functions
expit <- function(x) exp(x) / (1 + exp(x))
logit <- function(x) log( x / (1 - x))
# functions to estimate nuisance regressions
mu1.x <- function(y, x, new.x, sl.lib = c("SL.mean", "SL.lm", "SL.gam",
"SL.polymars", "SL.rpart")){
fit <- SuperLearner::SuperLearner(Y = y, X = x, SL.library = sl.lib,
newX = new.x)
return(fit$SL.predict)
}
mu.x <- mu0.x <- mu1.x
pi.x <- function(a, x, new.x, sl.lib = c("SL.mean", "SL.glm", "SL.ranger",
"SL.rpart")){
fit <- SuperLearner::SuperLearner(Y = a, X = x, SL.library = sl.lib,
newX = new.x, family = binomial())
return(fit$SL.predict)
}
drl.x <- function(y, x, new.x, sl.lib = c("SL.mean", "SL.lm", "SL.gam",
"SL.loess", "SL.glm", "SL.polymars",
"SL.rpart")){
fit <- SuperLearner::SuperLearner(Y = y, X = x, SL.library = sl.lib,
newX = new.x)
return(fit$SL.predict)
}
order_basis <- 10
lp <- legendre.polynomials(n = order_basis, normalized = TRUE)
lp.fns <- lapply(1:(order_basis + 1), function(u) as.function(lp[[u]]))
basis.legendre <- function(x, j) {
return(lp.fns[[j]](x))
}
h <- 0.1
kernel.gaussian <- function(x, x0) {
tmp <- function(u) prod(dnorm( as.matrix((u - x0)) / h) / h)
out <- apply(x, 1, tmp)
return(out)
}
# some generic parameters
n <- 2000
#################################
# Example 1 from Kennedy (2020) #
#################################
x0 <- seq(-0.8, 0.8, length.out = 101)
mu0_true <- function(y, x, new.x) {
u <- new.x[, 1]
(u <= -0.5) * 0.5 * (u + 2)^2 +
(u * 0.5 + 0.875) * (u > -0.5 & u < 0) +
(u > 0 & u < 0.5) * (-5 * (u - 0.2)^2 + 1.075) +
(u > .5) * (u + 0.125)
}
mu1_true <- function(y, x, new.x) {
mu0_true(y, x, new.x)
}
pix_true <- function(a, x, new.x){
u <- new.x[, 1]
0.1 + 0.8 * (u > 0)
}
gen_data <- function(n){
x <- data.frame(x = runif(n, -1, 1))
a <- rbinom(n, 1, pix_true(a = NULL, x = NULL, new.x = x))
y <- a * mu1_true(y = NULL, x = NULL, new.x = x) +
(1 - a) * mu0_true(y = NULL, x = NULL, new.x = x) +
rnorm(n, sd = (0.2 - 0.1 * cos(2 * pi * x$x)))
dat <- data.frame(y = y, a = a, x1 = x$x)
return(dat)
}
dat <- gen_data(n)
res_drl <- dr_learner(x0 = x0, y = dat$y, a = dat$a, x = dat[, -c(1:2), drop = FALSE],
drl.x = drl.x, mu1.x = mu1.x, mu0.x = mu0.x, pi.x = pi.x,
nsplits = 5)
res_lprl <- lp_r_learner(x0 = x0, y = dat$y, a = dat$a, x = dat[, -c(1:2), drop = FALSE],
mu.x = mu.x, pi.x = pi.x, basis = basis.legendre,
order_basis = order_basis, kernel = kernel.gaussian)
oracle <- dr_learner(x0 = x0, y = dat$y, a = dat$a, x = dat[, -c(1:2), drop = FALSE],
drl.x = drl.x, mu1.x = mu1_true, mu0.x = mu0_true,
pi.x = pix_true, nsplits = 5)
x0 <- x0
plot(x0, y = rep(0, length(x0)), type = "l", col = "red", ylim = c(-1.5, 1.5),
ylab = "E(Y^1 - Y^0)", xlab = "X0")
lines(x0, y = oracle$est, col = "blue")
lines(x0, y = res_drl$est, col = "black")
lines(x0, y = res_lprl$est, col = "pink")
legend(0.15, 1.5, legend = c("Truth", "Oracle", "DR-Learner", "Lp-R-Learner"),
col = c("red", "blue", "black", "pink"), cex = 0.8, lty = 1)
setwd("/Users/matteobonvini/Desktop/cate_estimation/drl.cate")
library(roxygen2)
library(devtools)
devtools::install_github("klutometis/roxygen")
document()
document()
document()
install.packages("/Users/matteobonvini/Desktop/cate_estimation/drl.cate",
repos = NULL,
type = "source")
?lp_r_learner
setwd("/Users/matteobonvini/Desktop/cate_estimation/drl.cate")
library(roxygen2)
library(devtools)
document()
source('~/Desktop/cate_estimation/drl.cate/R/examples.R')
document()
install.packages("/Users/matteobonvini/Desktop/cate_estimation/drl.cate",
repos = NULL,
type = "source")
setwd("/Users/matteobonvini/Desktop/cate_estimation/drl.cate")
library(roxygen2)
library(devtools)
document()
install.packages("/Users/matteobonvini/Desktop/cate_estimation/drl.cate",
repos = NULL,
type = "source")
library(drl.cate)
?dr_learner
